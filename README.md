### Automated Educational Question Generation at Different Bloomâ€™s Skill Levels using Large Language Models: Strategies and Evaluation

Developing questions that are pedagogically sound and relevant and promote learning is a challenging and time-consuming task for educators. Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators develop high-quality questions. Automated educational question generation (AEQG) can be crucial in scaling online education catering to a diverse student population. Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels. In this study, we examined the ability of five state-of-the-art LLMs of different sizes to generate relevant and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy. We used advanced prompting techniques with varying complexity for AEQG. We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions. Our findings suggest that LLMs can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although human experts remain superior in evaluating questions.
